{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snap \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import networkx as nx\n",
    "from datetime import datetime\n",
    "from matplotlib.pyplot import loglog \n",
    "\n",
    "%matplotlib inline \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProcessEdgeRow(row, Graph):\n",
    "    src = row['src']\n",
    "    dst = row['dst']\n",
    "    timestamp = row['timestamp']\n",
    "    \n",
    "    if not Graph.IsNode(src):\n",
    "        Graph.AddNode(src)\n",
    "    \n",
    "    if not Graph.IsNode(dst):\n",
    "        Graph.AddNode(dst)\n",
    "        \n",
    "    if not Graph.IsEdge(src, dst):\n",
    "        EId = Graph.AddEdge(src, dst)   \n",
    "    else:\n",
    "        EId = Graph.GetEI(src, dst) \n",
    "    \n",
    "    Graph.AddIntAttrDatE(EId, timestamp, 'timestamp')\n",
    "        \n",
    "    return\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateDirectedGraph(df):\n",
    "    '''\n",
    "    Returns a TNEANet Graph object \n",
    "    '''\n",
    "    Graph = snap.TNEANet.New()\n",
    "    df.apply(ProcessEdgeRow, axis=1, Graph=Graph)\n",
    "\n",
    "    return Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateGraph(filename='/home/merchantsameer2014/project/dnc-temporalGraph/out.dnc-temporalGraph'):\n",
    "    ''' read a list of edges \n",
    "        and return a snap multigraph\n",
    "    '''\n",
    "    df = pd.read_csv(filename, sep='\\t', header=None)\n",
    "    df.columns = ['src', 'dst', 'weight', 'timestamp']\n",
    "    G = GenerateDirectedGraph(df)\n",
    "    #print \"Nodes: \", G.GetNodes()\n",
    "    #print \"Edges: \", G.GetEdges()\n",
    "    return (df, G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotDegreeDistribution(G):\n",
    "    #\n",
    "    # Get Degree Distribution \n",
    "    # \n",
    "    OutDegToCntV = snap.TIntPrV()\n",
    "    snap.GetOutDegCnt(G, OutDegToCntV)\n",
    "    count = 0\n",
    "    nodeList = []\n",
    "    degreeList = []\n",
    "    for item in OutDegToCntV:\n",
    "        (n, d) = (item.GetVal2(), item.GetVal1())\n",
    "        nodeList.append(n)\n",
    "        degreeList.append(d)\n",
    "    x = np.array( [ np.log10(item.GetVal1()) for itemm in OutDegToCntV if item.GetVal1() > 0 ] )\n",
    "    y = np.array( [ np.log10(item.GetVal2()) for item in OutDegToCntV if item.GetVal2() > 0 ] )\n",
    "    #\n",
    "    # Plot Degree Distribution\n",
    "    #\n",
    "    plt.figure(figsize=(15,15))\n",
    "    loglog(degreeList, nodeList, 'bo')\n",
    "    #plt.plot(x_plot, 10**b*x_plot**a, 'r-')\n",
    "    plt.title(\"LogLog plot of out-degree distribution\")\n",
    "    plt.show()\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Compute Degree Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeDegreeCentrality(G, NodeAttributes):\n",
    "    #\n",
    "    # 1. Degree Centrality \n",
    "    #    Get In Degree and Out Degree for each node \n",
    "    #\n",
    "    InDegV = snap.TIntPrV()\n",
    "    OutDegV = snap.TIntPrV()\n",
    "\n",
    "    snap.GetNodeOutDegV(G, OutDegV)\n",
    "    snap.GetNodeInDegV(G, InDegV)\n",
    "\n",
    "    InDegreeList = [ (item.GetVal1(), item.GetVal2()) for item in InDegV ]\n",
    "    OutDegreeList = [ (item.GetVal1(), item.GetVal2()) for item in OutDegV ]\n",
    "    \n",
    "    InDegreeList.sort(key=lambda x: x[1], reverse=True)\n",
    "    OutDegreeList.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    minOutDegree = min(OutDegreeList, key = lambda x: x[1])[1]\n",
    "    maxOutDegree = max(OutDegreeList, key = lambda x: x[1])[1]\n",
    "    minInDegree = min(InDegreeList, key = lambda x: x[1])[1]\n",
    "    maxInDegree = max(InDegreeList, key = lambda x: x[1])[1]\n",
    "    \n",
    "    #\n",
    "    # Sanity Check \n",
    "    #print maxOutDegree, minOutDegree, maxInDegree, minInDegree\n",
    "    #print InDegreeList[0], InDegreeList[-1]\n",
    "    \n",
    "    for (nodeId, Degree) in InDegreeList:\n",
    "        if not NodeAttributes.get(nodeId, None):\n",
    "            NodeAttributes[nodeId] = dict()\n",
    "        NodeAttributes[nodeId]['InDegree'] = Degree\n",
    "        normalizedDegree = (float(Degree) - float(minInDegree))/(float(maxInDegree - float(minInDegree)))\n",
    "        NodeAttributes[nodeId]['NormInDegree'] = normalizedDegree\n",
    "\n",
    "    for (nodeId, Degree) in OutDegreeList: \n",
    "        NodeAttributes[nodeId]['OutDegree'] = Degree\n",
    "        normalizedDegree = (float(Degree) - float(minOutDegree))/(float(maxOutDegree - float(minOutDegree)))\n",
    "        NodeAttributes[nodeId]['NormOutDegree'] = normalizedDegree\n",
    "        \n",
    "    #\n",
    "    # Sanity Check \n",
    "    #\n",
    "    #print NodeAttributes[1874]\n",
    "    #print NodeAttributes[893]\n",
    "    \n",
    "    return NodeAttributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Compute Clustering Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeClusteringCoeff(G, NodeAttributes):\n",
    "    NIdCCfH = snap.TIntFltH()\n",
    "    snap.GetNodeClustCf(G, NIdCCfH)\n",
    "    \n",
    "    ClusterCoeffList = list()\n",
    "    for nodeId in NIdCCfH:\n",
    "        NodeAttributes[nodeId]['ClusterCoeff'] = NIdCCfH[nodeId]\n",
    "        ClusterCoeffList.append((nodeId, NIdCCfH[nodeId]))\n",
    "        \n",
    "    ClusterCoeffList.sort(key=lambda x: x[1], reverse=True)\n",
    "    minClusterCoeff = min(ClusterCoeffList, key=lambda x: x[1])[1]\n",
    "    maxClusterCoeff = max(ClusterCoeffList, key=lambda x: x[1])[1]\n",
    "    \n",
    "    #\n",
    "    # Sanity Check \n",
    "    #\n",
    "    print ClusterCoeffList[1], maxClusterCoeff, ClusterCoeffList[-1], minClusterCoeff\n",
    "    \n",
    "    NIdCCfH = snap.TIntFltH()\n",
    "    snap.GetNodeClustCf(G, NIdCCfH)\n",
    "    ClusterCoeffList = list()\n",
    "    for nodeId in NIdCCfH:\n",
    "        clusterCoeff = NIdCCfH[nodeId]\n",
    "        normClusterCoeff = (clusterCoeff - minClusterCoeff)/(maxClusterCoeff - minClusterCoeff)\n",
    "        NodeAttributes[nodeId]['NormClusterCoeff'] = normClusterCoeff\n",
    "        \n",
    "    #print NodeAttributes[2012]\n",
    "    return NodeAttributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Compute Avergate shortest path length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeAvgShortestPath(G, NodeAttributes):\n",
    "    nodeCount = float(G.GetNodes() - 1)\n",
    "    avgShortPathLenList = list()\n",
    "    for src in G.Nodes():\n",
    "        srcId = src.GetId()\n",
    "        totalShortPathLength = 0 \n",
    "    \n",
    "        for dst in G.Nodes():\n",
    "            dstId = dst.GetId()\n",
    "            #\n",
    "            # Skip Self Edges\n",
    "            #\n",
    "            if srcId == dstId:\n",
    "                continue\n",
    "            \n",
    "            #\n",
    "            # Compute Shortest Path \n",
    "            #\n",
    "            l = snap.GetShortPath(G, srcId, dstId, True)\n",
    "        \n",
    "            #\n",
    "            # Skip nodes that cannot be reached \n",
    "            #\n",
    "            if l < 0:\n",
    "                continue\n",
    "            \n",
    "            totalShortPathLength += float(l) \n",
    "        NodeAttributes[srcId]['AvgShortPathLen'] = totalShortPathLength/nodeCount\n",
    "        avgShortPathLenList.append((srcId, totalShortPathLength/nodeCount))\n",
    "        \n",
    "    minAvgShortPathLength = min(avgShortPathLenList, key=lambda x: x[1])[1]\n",
    "    maxAvgShortPathLength = max(avgShortPathLenList, key=lambda x: x[1])[1]\n",
    "    \n",
    "    for (node, spLen) in avgShortPathLenList:\n",
    "        normAvgShortPath = (spLen - minAvgShortPathLength)/(maxAvgShortPathLength - minAvgShortPathLength)\n",
    "        NodeAttributes[node]['normAvgShortPathLen'] = normAvgShortPath\n",
    "    \n",
    "    #print NodeAttributes[480]\n",
    "    \n",
    "    return NodeAttributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Compute Betweeness Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeBetweenessCentrality(G, NodeAttributes):\n",
    "    Nodes = snap.TIntFltH()\n",
    "    Edges = snap.TIntPrFltH()\n",
    "    BetweenessNodeList = list()\n",
    "    BetweenessEdgeList = list()\n",
    "\n",
    "    snap.GetBetweennessCentr(G, Nodes, Edges, 1.0)\n",
    "    for node in Nodes:\n",
    "        NodeAttributes[node]['Betweeness'] = Nodes[node]\n",
    "        BetweenessNodeList.append((node, Nodes[node]))\n",
    "\n",
    "    for edge in Edges:\n",
    "        #print \"edge: (%d, %d) centrality: %f\" % (edge.GetVal1(), edge.GetVal2(), Edges[edge])\n",
    "        BetweenessEdgeList.append((edge.GetVal1(), edge.GetVal2(), Edges[edge]))\n",
    "\n",
    "    BetweenessNodeList.sort(key=lambda x: x[1], reverse=True) \n",
    "    BetweenessEdgeList.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    #print BetweenessNodeList[0], BetweenessNodeList[-1]\n",
    "    \n",
    "    minBetweeness = BetweenessNodeList[-1][1]\n",
    "    maxBetweeness = BetweenessNodeList[0][1]\n",
    "    for (node, betweeness) in BetweenessNodeList:\n",
    "        normBetweeness = (betweeness - minBetweeness)/(maxBetweeness - minBetweeness)\n",
    "        NodeAttributes[node]['normBetweeness'] = normBetweeness\n",
    "        \n",
    "    #print NodeAttributes[1669]\n",
    "    #print NodeAttributes[884]\n",
    "    \n",
    "    return NodeAttributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.  Compute Auth and Hub Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeAuthHubScore(G, NodeAttributes):\n",
    "    NIdHubH = snap.TIntFltH()\n",
    "    NIdAuthH = snap.TIntFltH()\n",
    "    snap.GetHits(G, NIdHubH, NIdAuthH)\n",
    "    HubNodes = []\n",
    "    for nodeId in  NIdHubH:\n",
    "        HubNodes.append((nodeId,  NIdHubH[nodeId]))\n",
    "        NodeAttributes[nodeId]['HubScore'] = NIdHubH[nodeId]\n",
    "    \n",
    "    HubNodes.sort(key = lambda x: x[1], reverse=True)\n",
    "\n",
    "    AuthNodes = []\n",
    "    for nodeId in  NIdAuthH:\n",
    "        AuthNodes.append((nodeId,  NIdAuthH[nodeId])) \n",
    "        NodeAttributes[nodeId]['AuthScore'] = NIdAuthH[nodeId]\n",
    "        \n",
    "    AuthNodes.sort(key = lambda x: x[1], reverse=True)\n",
    "\n",
    "    #print AuthNodes[0], AuthNodes[-1]\n",
    "    #print HubNodes[0], HubNodes[-1]\n",
    "    \n",
    "    minAuthNodes = AuthNodes[-1][1]\n",
    "    maxAuthNodes = AuthNodes[0][1]\n",
    "    minHubNodes = HubNodes[-1][1]\n",
    "    maxHubNodes = HubNodes[0][1]\n",
    "    \n",
    "    for (node, hubScore) in HubNodes:\n",
    "        normHubScore = (hubScore - minHubNodes)/(maxHubNodes - minHubNodes)\n",
    "        NodeAttributes[node]['normHubScore'] = normHubScore\n",
    "    \n",
    "    for (node, authScore) in AuthNodes:\n",
    "        normAuthScore = (authScore - minAuthNodes)/(maxAuthNodes - minAuthNodes)\n",
    "        NodeAttributes[node]['normAuthScore'] = normAuthScore\n",
    "    \n",
    "    #print NodeAttributes[1874]\n",
    "    #print NodeAttributes[893]\n",
    "    return NodeAttributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Raw Statistics \n",
    "\n",
    "    1. Degree Centrality \n",
    "    2. Clustering Coefficient \n",
    "    3. Mean shortest path from each node \n",
    "    4. Betweeness Centralitily \n",
    "    5. Hub and Authority Score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetRawStatistics(G, NodeAttributes):\n",
    "    \n",
    "    #\n",
    "    # Step 1: Compute Degree Centrality\n",
    "    #\n",
    "    print \"Computing Degree Centrality... \"\n",
    "    computeDegreeCentrality(G, NodeAttributes)\n",
    "    \n",
    "    #\n",
    "    # Step 2: Compute Clustering Coefficients \n",
    "    #\n",
    "    print \"Computing Clustering coefficient...\"\n",
    "    computeClusteringCoeff(G, NodeAttributes)\n",
    "    \n",
    "    #\n",
    "    # Step 3: Avg. Shortest path length \n",
    "    #\n",
    "    print \"Computing Average Shortest path length ... \"\n",
    "    computeAvgShortestPath(G, NodeAttributes)\n",
    "    \n",
    "    # \n",
    "    # Step 4: Betweeness Centrality\n",
    "    #\n",
    "    print \"Computing Betweeness centrality... \"\n",
    "    computeBetweenessCentrality(G, NodeAttributes)\n",
    "    \n",
    "    #\n",
    "    # Step 5: Hub and Auth Score \n",
    "    #\n",
    "    print \"Computing Hub and Auth Score ... \"\n",
    "    computeAuthHubScore(G, NodeAttributes)\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Social Hierarchy Detection From Email Network \n",
    "\n",
    "### Steps \n",
    "\n",
    "    1. Compute node's importance on response time for email  \n",
    "    2. Get all Cliques (Algorithm 457)\n",
    "    3. Number of clique each node is part of \n",
    "    4. Raw Clique Score computed using \n",
    "    \n",
    " $$R*2^{n-1}$$\n",
    " \n",
    "    5. Weighted Clique Score (Based on importance of a person) \n",
    "    \n",
    " $$W = t*2^{n-1}$$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use networkx library for Clique analysis \n",
    "\n",
    "    1. Build a multigraph with edges for each (src, dst, timestamp) entry in the email data set \n",
    "    2. Build undirected graph with edges between nodes only when email count exceeds N=4 between two those nodes \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProcessNxEdgeRow(row, Graph):\n",
    "    src = row['src']\n",
    "    dst = row['dst']\n",
    "    timestamp = row['timestamp']\n",
    "    \n",
    "    Graph.add_node(src)\n",
    "    Graph.add_node(dst)\n",
    "    Graph.add_edge(src, dst, timestamp=timestamp)\n",
    "        \n",
    "    return\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateDirectedNxGraph(df):\n",
    "    '''\n",
    "    Returns a TNEANet Graph object \n",
    "    '''\n",
    "    Graph = nx.MultiDiGraph(name=\"DNC Email Network\")\n",
    "    df.apply(ProcessNxEdgeRow, axis=1, Graph=Graph)\n",
    "\n",
    "    #print \"Networkx Nodes: \", Graph.number_of_nodes()\n",
    "    #print \"Networkx Edges: \", Graph.number_of_edges()\n",
    "    \n",
    "    return Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate undirected graph with edges between nodes with email count > N\n",
    "\n",
    "#### Prune all edges with N <=4 emails exchanged\n",
    "As per the paper - consider edges between nodes only if the nodes have exchanged > N messages\n",
    "N is a tunable parameter \n",
    "\n",
    "DNC email has a median of message exchanged = 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GeneratePrunedDirectedGraph(GNx,  N = 4):\n",
    "    \n",
    "    edgeCount = dict()\n",
    "    for edge in GNx.edges():\n",
    "        if not edgeCount.get(edge, None):\n",
    "            edgeCount[edge] = 0\n",
    "        edgeCount[edge] += 1 \n",
    "        \n",
    "    emailDist= [ v for k, v in edgeCount.iteritems() ]\n",
    "    emailDist.sort(reverse=True)\n",
    "    #print emailDist[0], emailDist[-1], np.median(emailDist), len(emailDist)\n",
    "    \n",
    "   \n",
    "    pruneList = [ k for k, v in edgeCount.iteritems() if v <= N ]\n",
    "    prunedEdgeList = [ k for k, v in edgeCount.iteritems() if v > N ]\n",
    "    \n",
    "    uGNx = nx.Graph()\n",
    "\n",
    "    for edge in prunedEdgeList:\n",
    "        (src, dst) = edge\n",
    "        uGNx.add_edge(int(src), int(dst))\n",
    "    \n",
    "    #print \"Networkx Nodes: \", uGNx.number_of_nodes()\n",
    "    #print \"Networkx Edges: \", uGNx.number_of_edges() \n",
    "    \n",
    "    return uGNx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Compute Nodes importance on email response time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1: Based on Rowe et. al. paper \n",
    "\n",
    "Use response time to measure importance of a node \n",
    "\n",
    "    1. For each node get all the out bound email timestamps \n",
    "    2. For each email sent - check the response time from the receiver \n",
    "    3. Consider responses within a day for computing time score. \n",
    "    4. Consider response time for nodes that have exchanged at least 100 emails \n",
    "       (fewer emails with high response time can falsely promote node based on time score)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmailSentByNode(GNx):\n",
    "    temporalMap = dict()\n",
    "    for n, nbrs in GNx.adjacency():\n",
    "        temporalMap[n] = dict()\n",
    "        for nbr, edict in nbrs.items():\n",
    "            t1 = [ d['timestamp'] for d in edict.values() ]\n",
    "            t1.sort()\n",
    "            temporalMap[n][nbr] = t1 \n",
    "            \n",
    "    return temporalMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAvgResponseTime(temporalMap):\n",
    "    avgNodeResponse = list()\n",
    "\n",
    "    for src, destinations in temporalMap.iteritems():\n",
    "        totalRequestResp = 0\n",
    "        totalResponseTime = 0.0\n",
    "    \n",
    "        for dst, reqTimestamps in destinations.iteritems():\n",
    "            responseTime = None\n",
    "            #\n",
    "            # Walk through ALl requests sent to a destination\n",
    "            #\n",
    "            for req in reqTimestamps:\n",
    "                reqTime = datetime.fromtimestamp(req)\n",
    "        \n",
    "                #\n",
    "                # Look for response time from the destinaton to ths source\n",
    "                #\n",
    "                for resp in temporalMap[dst].get(src, list()):\n",
    "                    respTime = datetime.fromtimestamp(resp)\n",
    "            \n",
    "                    if resp < req:\n",
    "                        #\n",
    "                        # Look for first response after the request time \n",
    "                        #\n",
    "                        continue\n",
    "            \n",
    "                    deltaTime = respTime - reqTime\n",
    "            \n",
    "                    if deltaTime.total_seconds() > 86400:\n",
    "                        #\n",
    "                        # If response time exceeds a day don't \n",
    "                        # consider it as a response\n",
    "                        break\n",
    "                    \n",
    "                    #print \"Found Response time: src: %d, dst: %d req: %s, resp: %s\" % (src, dst, reqTime, respTime)\n",
    "                \n",
    "                    totalRequestResp += 1\n",
    "                    totalResponseTime += deltaTime.total_seconds()\n",
    "                    break\n",
    "        #\n",
    "        # Compute average across all dst response times \n",
    "        #\n",
    "        if totalRequestResp > 0 and totalResponseTime > 0:\n",
    "            avgResponse = totalResponseTime/totalRequestResp\n",
    "        else:\n",
    "            #\n",
    "            # Set default response to 7 day \n",
    "            #\n",
    "            avgResponse = float(7*86400)\n",
    "        #\n",
    "        # Lower response Higher the timeScore \n",
    "        # \n",
    "        timeScore = 1/avgResponse\n",
    "        avgNodeResponse.append((src, timeScore, totalResponseTime, totalRequestResp ))\n",
    "        \n",
    "    avgNodeTimeScore = list()\n",
    "\n",
    "    #\n",
    "    # Ignore response time for email exchanges fewer than 10\n",
    "    #\n",
    "    for x in avgNodeResponse:\n",
    "        (src, timeScore, totalResponseTime, totalRequestResp) = x\n",
    "    \n",
    "        if totalRequestResp <= 200:\n",
    "            timeScore = 1.0/float(7*86400)\n",
    "        \n",
    "        avgNodeTimeScore.append((src, timeScore, totalResponseTime, totalRequestResp))\n",
    "        \n",
    "    avgNodeTimeScore.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    #print \"\\nLast Node avg response time: \", avgNodeTimeScore[-1]\n",
    "    #print \"\\nTop top two nodes avg response time:\", avgNodeTimeScore[:2] \n",
    "    \n",
    "    normTimeScore = dict()\n",
    "    minAvgTimeScore = avgNodeTimeScore[-1][1]\n",
    "    maxAvgTimeScore = avgNodeTimeScore[0][1]\n",
    "\n",
    "    for (node, timeScore, _, _) in avgNodeTimeScore:\n",
    "        normTimeScore[node] = (timeScore - minAvgTimeScore)/(maxAvgTimeScore - minAvgTimeScore)\n",
    "    \n",
    "    #print \"Node 1625 normTimeScore: \", normTimeScore[1625]\n",
    "    \n",
    "    return normTimeScore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetSocialAttributes(df, NodeAttributes, threshold=0):\n",
    "    \n",
    "    # Generate NetworkX graph\n",
    "    #\n",
    "    GNx = GenerateDirectedNxGraph(df)\n",
    "    \n",
    "    #\n",
    "    # Prune graph based on number of emails exchanged \n",
    "    # Keep edges with edge weight > 4 (exchanged > 4 emails)\n",
    "    #\n",
    "\n",
    "    uGNx = GeneratePrunedDirectedGraph(GNx, N=threshold)\n",
    "    \n",
    "    temporalMap = getEmailSentByNode(GNx)\n",
    "    normTimeScore = getAvgResponseTime(temporalMap)\n",
    "    \n",
    "    nodeCliqueCount = dict()\n",
    "    rawCliqueScore = dict()\n",
    "    weightedCliqueScore = dict()\n",
    "\n",
    "    #\n",
    "    # Find all maximal cliques \n",
    "    # \n",
    "    for clique in nx.find_cliques(uGNx):\n",
    "        for node in clique:\n",
    "        \n",
    "            if not nodeCliqueCount.get(node, None):\n",
    "                nodeCliqueCount[node] = 0\n",
    "            \n",
    "            if not rawCliqueScore.get(node, None):\n",
    "                rawCliqueScore[node] = 0\n",
    "        \n",
    "            if not weightedCliqueScore.get(node, None):\n",
    "                weightedCliqueScore[node] = 0\n",
    "            \n",
    "            nodeCliqueCount[node] += 1 \n",
    "        \n",
    "            n = len(clique)\n",
    "        \n",
    "            rawCliqueScore[node] += 2**n\n",
    "        \n",
    "            weightedCliqueScore[node] += (2**n)*normTimeScore[node]\n",
    "    #\n",
    "    # Get a sorted list of nodes based on their clique count \n",
    "    #\n",
    "    nodeCliqueList = [ (k, v) for k, v in nodeCliqueCount.iteritems() ]\n",
    "    nodeCliqueList.sort(key=lambda x: x[1], reverse=True )\n",
    "    \n",
    "    rawCliqueList = [ (k, v) for k, v in rawCliqueScore.iteritems() ]\n",
    "    rawCliqueList.sort(key=lambda x: x[1], reverse=True )\n",
    "    \n",
    "    weightedCliqueList = [ (k, v) for k, v in weightedCliqueScore.iteritems() ]\n",
    "    weightedCliqueList.sort(key=lambda x: x[1], reverse=True )\n",
    "    \n",
    "    #print \"\\n Top 10 nodes based on Cliques count: \" , nodeCliqueList[:10]\n",
    "    #print \"\\n Top 10 nodes based on Raw Clique size: \",  rawCliqueList[:10]\n",
    "    #print \"\\n Top 10 nodes based on weighted Clique size: \" , weightedCliqueList[:10]\n",
    "    \n",
    "    minNodeCliqueCount = nodeCliqueList[-1][1]\n",
    "    maxNodeCliqueCount = nodeCliqueList[0][1]\n",
    "\n",
    "    minRawCliqueScore = rawCliqueList[-1][1]\n",
    "    maxRawCliqueScore = rawCliqueList[0][1]\n",
    "\n",
    "    minWeightedCliqueScore = weightedCliqueList[-1][1]\n",
    "    maxWeightedCliqueScore = weightedCliqueList[0][1]\n",
    "    \n",
    "    for node, score in nodeCliqueList:\n",
    "        NodeAttributes[node]['nodeClique'] = score\n",
    "        NodeAttributes[node]['normNodeClique'] = float(score - minNodeCliqueCount)/float(maxNodeCliqueCount - minNodeCliqueCount)\n",
    "        \n",
    "    for node, score in rawCliqueList:\n",
    "        NodeAttributes[node]['rawClique'] = score\n",
    "        NodeAttributes[node]['normRawClique'] = float(score - minRawCliqueScore)/float(maxRawCliqueScore - minRawCliqueScore)\n",
    "    \n",
    "    for node, score in weightedCliqueList:\n",
    "        NodeAttributes[node]['weightedClique'] = score\n",
    "        NodeAttributes[node]['normWeightedClique'] = float(score - minWeightedCliqueScore)/float(maxWeightedCliqueScore - minWeightedCliqueScore)\n",
    "     \n",
    "    #print \"\\nNode 1874 features: \", NodeAttributes[1874]\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modified version of Social Score\n",
    "\n",
    "Social hierarchy can be determined by how often a person receives a response to their mail within first 10 mails sent by the receipient. Instead of using time as a measure which can be skewed due to different time zones or working hours, a priority of mails can be a better indicator of a persons importance in the organization \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNodeResponseList(GNx):\n",
    "    nodeResponseTuple = dict()\n",
    "    nodeAdj = dict()\n",
    "    \n",
    "    for n, nbrs in GNx.adjacency():\n",
    "        nodeAdj[n] = dict()\n",
    "        nodeResponseTuple[n] = list()\n",
    "        for nbr, edict in nbrs.items():\n",
    "            t1 = [ d['timestamp'] for d in edict.values() ]\n",
    "            t2 = [nbr]*len(t1)\n",
    "            response = zip(t1,t2)\n",
    "            response.sort(key=lambda x: x[0])\n",
    "            nodeResponseTuple[n].extend(response)\n",
    "            \n",
    "            if nbr not in nodeAdj[n]:\n",
    "                nodeAdj[n][nbr] = list()\n",
    "            nodeAdj[n][nbr].extend(t1)\n",
    "            \n",
    "    return nodeAdj, nodeResponseTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAvgResponseScore(nodeAdj, nodeResponseTuple):\n",
    "    avgNodeResponse = list()\n",
    "\n",
    "    for src, destinations in nodeAdj.iteritems():\n",
    "        totalRequests = 0.0\n",
    "        totalPriorityResponse = 0.0\n",
    "        #print \"Processing %r\" % src\n",
    "        for dst, reqTimestamps in destinations.iteritems():\n",
    "            #\n",
    "            # Walk through ALl requests sent to a destination\n",
    "            #         \n",
    "            for req in reqTimestamps:\n",
    "                totalRequests += 1        \n",
    "                #\n",
    "                # check if destination responded to src within next 5 mails \n",
    "                #\n",
    "                count = 0\n",
    "                found_response = False\n",
    "                for resp, node in nodeResponseTuple[dst]:            \n",
    "                    if resp < req:\n",
    "                        #\n",
    "                        # Look for first response after the request time \n",
    "                        #\n",
    "                        continue\n",
    "                    \n",
    "                    count += 1\n",
    "                    found_response = True\n",
    "                    if node == src:\n",
    "                        #print \"Found a response in top 5..\"\n",
    "                        break\n",
    "                    \n",
    "                    if count > 5:\n",
    "                        break\n",
    "                \n",
    "                if found_response and (count <= 5):\n",
    "                    totalPriorityResponse += 1\n",
    "                    break\n",
    "        #\n",
    "        # Compute average across all dst response times \n",
    "        # \n",
    "        #\n",
    "        #print \"src: %r, total mails sent %r, priority: %r\" % (src, totalRequests, totalPriorityResponse)\n",
    "        if totalRequests > 0 and totalPriorityResponse > 0:\n",
    "            avgResponse = totalPriorityResponse/totalRequests\n",
    "        else:\n",
    "            #\n",
    "            # Set default priority respone to zero\n",
    "            #\n",
    "            avgResponse = 0\n",
    "\n",
    "        avgNodeResponse.append((src, avgResponse, totalRequests, totalPriorityResponse ))\n",
    "        \n",
    "    avgNodeResponseScore = list()\n",
    "\n",
    "    #\n",
    "    # Ignore response time for email exchanges fewer than 200\n",
    "    #\n",
    "    for x in avgNodeResponse:\n",
    "        (src, avgResponse, totalRequests, totalPriorityResponse) = x\n",
    "    \n",
    "        if totalRequests <= 200:\n",
    "            avgResponse = 0\n",
    "        \n",
    "        avgNodeResponseScore.append((src, avgResponse, totalRequests, totalPriorityResponse))\n",
    "        \n",
    "    avgNodeResponseScore.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    #print \"\\nLast Node avg response score: \", avgNodeResponseScore[-1]\n",
    "    print \"\\nTop top nodes avg response score:\", avgNodeResponseScore[:10] \n",
    "    \n",
    "    \n",
    "    normResponseScore = dict()\n",
    "    minAvgResponseScore = avgNodeResponseScore[-1][1]\n",
    "    maxAvgResponseScore = avgNodeResponseScore[0][1]\n",
    "\n",
    "    for (node, responseScore, _, _) in avgNodeResponseScore:\n",
    "        normResponseScore[node] = (responseScore - minAvgResponseScore)/(maxAvgResponseScore - minAvgResponseScore)\n",
    "    \n",
    "    #print \"\\nNode 1625 normTimeScore: \", normResponseScore[1625]\n",
    "    \n",
    "    return normResponseScore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetModifiedSocialAttributes(df, NodeAttributes, threshold=0):\n",
    "    \n",
    "    # Generate NetworkX graph\n",
    "    #\n",
    "    GNx = GenerateDirectedNxGraph(df)\n",
    "    \n",
    "    #\n",
    "    # Prune graph based on number of emails exchanged \n",
    "    # Keep edges with edge weight > 4 (exchanged > 4 emails)\n",
    "    #\n",
    "\n",
    "    uGNx = GeneratePrunedDirectedGraph(GNx, N=threshold)\n",
    "    \n",
    "    nodeAdj, nodeResponseTuple = getNodeResponseList(GNx)\n",
    "    normResponseScore = getAvgResponseScore(nodeAdj, nodeResponseTuple)\n",
    "    \n",
    "    nodeCliqueCount = dict()\n",
    "    rawCliqueScore = dict()\n",
    "    weightedCliqueScore = dict()\n",
    "\n",
    "    #\n",
    "    # Find all maximal cliques \n",
    "    # \n",
    "    for clique in nx.find_cliques(uGNx):\n",
    "        for node in clique:\n",
    "        \n",
    "            if not nodeCliqueCount.get(node, None):\n",
    "                nodeCliqueCount[node] = 0\n",
    "            \n",
    "            if not rawCliqueScore.get(node, None):\n",
    "                rawCliqueScore[node] = 0\n",
    "        \n",
    "            if not weightedCliqueScore.get(node, None):\n",
    "                weightedCliqueScore[node] = 0\n",
    "            \n",
    "            nodeCliqueCount[node] += 1 \n",
    "        \n",
    "            n = len(clique)\n",
    "        \n",
    "            rawCliqueScore[node] += 2**n\n",
    "        \n",
    "            weightedCliqueScore[node] += (2**n)*normResponseScore[node]\n",
    "    #\n",
    "    # Get a sorted list of nodes based on their clique count \n",
    "    #\n",
    "    nodeCliqueList = [ (k, v) for k, v in nodeCliqueCount.iteritems() ]\n",
    "    nodeCliqueList.sort(key=lambda x: x[1], reverse=True )\n",
    "    \n",
    "    rawCliqueList = [ (k, v) for k, v in rawCliqueScore.iteritems() ]\n",
    "    rawCliqueList.sort(key=lambda x: x[1], reverse=True )\n",
    "    \n",
    "    weightedCliqueList = [ (k, v) for k, v in weightedCliqueScore.iteritems() ]\n",
    "    weightedCliqueList.sort(key=lambda x: x[1], reverse=True )\n",
    "    \n",
    "    #print \"\\n Top 10 nodes based on Cliques count: \" , nodeCliqueList[:10]\n",
    "    #print \"\\n Top 10 nodes based on Raw Clique size: \",  rawCliqueList[:10]\n",
    "    print \"\\n Top 10 nodes based on weighted Clique size: \" , weightedCliqueList[:10]\n",
    "    \n",
    "    minNodeCliqueCount = nodeCliqueList[-1][1]\n",
    "    maxNodeCliqueCount = nodeCliqueList[0][1]\n",
    "\n",
    "    minRawCliqueScore = rawCliqueList[-1][1]\n",
    "    maxRawCliqueScore = rawCliqueList[0][1]\n",
    "\n",
    "    minWeightedCliqueScore = weightedCliqueList[-1][1]\n",
    "    maxWeightedCliqueScore = weightedCliqueList[0][1]\n",
    "    \n",
    "    for node, score in nodeCliqueList:\n",
    "        if not NodeAttributes.get(node, None):\n",
    "            NodeAttributes[node] = dict()\n",
    "        NodeAttributes[node]['nodeClique'] = score\n",
    "        NodeAttributes[node]['normNodeClique'] = float(score - minNodeCliqueCount)/float(maxNodeCliqueCount - minNodeCliqueCount)\n",
    "        \n",
    "    for node, score in rawCliqueList:\n",
    "        NodeAttributes[node]['rawClique'] = score\n",
    "        NodeAttributes[node]['normRawClique'] = float(score - minRawCliqueScore)/float(maxRawCliqueScore - minRawCliqueScore)\n",
    "    \n",
    "    for node, score in weightedCliqueList:\n",
    "        NodeAttributes[node]['weightedClique'] = score\n",
    "        NodeAttributes[node]['normWeightedClique'] = float(score - minWeightedCliqueScore)/float(maxWeightedCliqueScore - minWeightedCliqueScore)\n",
    "    \n",
    "    #print \"\\nNode 1874 features: \", NodeAttributes[1874]\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Weighted Social Score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Social Score is computed as follows \n",
    "\n",
    "\\begin{align}\n",
    "w_x * C_x & = w_x * 100 * \\left[ \\frac{x_i - infx}{sup\\ x - inf\\ x} \\right] \\\\\n",
    "\\\\\n",
    "score & = \\frac{\\Sigma_{all\\ x} w * C_x}{\\Sigma_{all\\ x} w}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeSocialScore(NodeAttributes):\n",
    "    \n",
    "    socialScore = list()\n",
    "    \n",
    "    #\n",
    "    # Weights:\n",
    "    #\n",
    "    # W = [ Weighted Clique, RawClique, NumClique, OutDegree, InDegree, Cluster Coeff, Betweeness, Avg ShortPath, Auth, Hub]\n",
    "    #\n",
    "    w_weightedClique = 0.9\n",
    "    w_rawClique = 0.8\n",
    "    w_numClique = 0.7\n",
    "    w_outDegree = 0.6\n",
    "    w_inDegree = 0.6\n",
    "    w_clusterCoeff = 0.3\n",
    "    w_betweeness = 0.3 \n",
    "    w_shortpath = 0.5 \n",
    "    w_auth = 0.2 \n",
    "    w_hub = 0.1 \n",
    "\n",
    "    w = np.array([w_weightedClique,\\\n",
    "                 w_rawClique, \\\n",
    "                 w_numClique, \\\n",
    "                 w_outDegree, \\\n",
    "                 w_inDegree, \\\n",
    "                 w_clusterCoeff, \\\n",
    "                 w_betweeness, \\\n",
    "                 w_shortpath, \\\n",
    "                 w_auth, \\\n",
    "                 w_hub])\n",
    "\n",
    "    w_sum = np.sum(w)\n",
    "\n",
    "    for node, attributes in NodeAttributes.iteritems():\n",
    "        weightedClique = attributes.get('normWeightedClique', 0.0)\n",
    "        rawClique = attributes.get('normRawClique', 0.0)\n",
    "        numClique = attributes.get('normNodeClique', 0.0)\n",
    "        outDeg =  attributes.get('NormOutDegree', 0.0)\n",
    "        inDeg =  attributes.get('NormInDegree', 0.0)\n",
    "        clusterCoeff =  attributes.get('NormClusterCoeff', 0.0)\n",
    "        betweeness = attributes.get('normBetweeness', 0.0)\n",
    "        shortestPath = attributes.get('normAvgShortPathLen', 0.0)\n",
    "        authScore = attributes.get('normAuthScore', 0.0)\n",
    "        hubSccore = attributes.get('normHubScore', 0.0)\n",
    "    \n",
    "        C_x = np.array([weightedClique, \\\n",
    "                       rawClique, \\\n",
    "                       numClique, \\\n",
    "                       outDeg, \\\n",
    "                       inDeg, \\\n",
    "                       clusterCoeff, \\\n",
    "                       betweeness, \\\n",
    "                       shortestPath, \\\n",
    "                       authScore, \\\n",
    "                       hubSccore])\n",
    "    \n",
    "        score = 100.0 * np.dot(w, C_x)/w_sum\n",
    "        socialScore.append((node, score))\n",
    "        \n",
    "    socialScore.sort(key=lambda x: x[1], reverse=True)\n",
    "    print socialScore[:10]\n",
    "    \n",
    "    return socialScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeNodeScore(df, G, modified=False, threshold=0):\n",
    "    #\n",
    "    # Create a Node Attribute Dictionary\n",
    "    #\n",
    "    NodeAttributes = dict()\n",
    "    \n",
    "    #\n",
    "    # Get Node Statistics and Graph attributes\n",
    "    #\n",
    "    GetRawStatistics(G, NodeAttributes)\n",
    "    \n",
    "    #\n",
    "    # Get Social Attributes for nodes \n",
    "    #\n",
    "    if modified:\n",
    "        print \"Get Modified Social Score Attributes... \"\n",
    "        GetModifiedSocialAttributes(df, NodeAttributes, threshold=threshold) \n",
    "    else:\n",
    "        print \"Get Social Score Attributes... \"\n",
    "        GetSocialAttributes(df, NodeAttributes, threshold=threshold)\n",
    "    \n",
    "    print \"Compute Social Score... \"\n",
    "    socialScore = ComputeSocialScore(NodeAttributes)\n",
    "\n",
    "    return socialScore, NodeAttributes \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Degree Centrality... \n",
      "Computing Clustering coefficient...\n",
      "(1753, 1.0) 1.0 (1381, 0.0) 0.0\n",
      "Computing Average Shortest path length ... \n",
      "Computing Betweeness centrality... \n",
      "Computing Hub and Auth Score ... \n",
      "Get Social Score Attributes... \n",
      "Compute Social Score... \n",
      "[(1874, 80.78021754303406), (1258, 55.4659080081707), (999, 51.65134719039465), (1669, 42.936504335383276), (511, 41.144926456992756), (1998, 38.94771394756082), (585, 38.78737308016589), (1440, 34.7177822103923), (852, 27.237611856350544), (1278, 26.892696751894427)]\n",
      "Total time to compute Social Score: 0:06:06.993746\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    df, G = GenerateGraph(filename='/home/merchantsameer2014/project/dnc-temporalGraph/out.dnc-temporalGraph')\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    socialScore, NodeAttributes = ComputeNodeScore(df, G, modified=False, threshold=0)\n",
    "    end_time = datetime.now()\n",
    "    print('Total time to compute Social Score: {}'.format(end_time - start_time))\n",
    "\n",
    "    \n",
    "    with open('socialScoreModifed_w07.txt', 'w') as fd:\n",
    "        for (node, score) in socialScore:\n",
    "            fd.write(\"%r\\t%r\\n\" % (node, score))\n",
    "    \n",
    "    '''\n",
    "    with open('../results/nodeSocialScoreFeature.txt', 'w') as fd:\n",
    "        fd.write(\"node,normNodeClique,normRawClique,normWeightedClique,nodeClique,rawClique,weightedClique\\n\")\n",
    "        for (node, attributes) in NodeAttributes.iteritems():\n",
    "            fd.write(\"%r,%r,%r,%r,%r,%r,%r\\n\" % (node,\\\n",
    "                                        attributes.get('normNodeClique', 0.0),\\\n",
    "                                        attributes.get('normRawClique', 0.0),\\\n",
    "                                        attributes.get('normWeightedClique', 0.0),\\\n",
    "                                        attributes.get('nodeClique', 0.0),\\\n",
    "                                        attributes.get('rawClique', 0.0),\\\n",
    "                                        attributes.get('weigthedClique', 0.0),\\\n",
    "                                       ))\n",
    "          \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2 (project env)",
   "language": "python",
   "name": "project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
